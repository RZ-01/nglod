
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: center;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

</style>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes</title>
        <meta property="og:description" content="Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Surfaces"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@yongyuanxi">
        <meta name="twitter:title" content="Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Surfaces">
        <meta name="twitter:description" content="Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. SDFs encode 3D surfaces with a function of position that returns the closest distance to a surface. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics applications. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.">
        <meta name="twitter:image" content="https://nv-tlabs.github.io/nglod/assets/teaser_small.jpg">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-6HHDEXF452');
    </script>

    </head>

 <body>
<div class="container">
    <div class="paper-title">
      <h1>Neural Geometric Level of Detail:<br>Real-time Rendering with Implicit 3D Shapes</h1>
    </div>

    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://tovacinni.github.io">Towaki Takikawa</a><sup>1,2,4*</sup></div>
            <div class="col-5 text-center"><a href="https://joeylitalien.github.io">Joey Litalien</a><sup>1,3*</sup></div>
            <div class="col-5 text-center"><a href="https://kangxue.org/">Kangxue Yin</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://scholar.google.de/citations?user=rFd-DiAAAAAJ">Karsten Kreis</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://research.nvidia.com/person/charles-loop">Charles Loop</a><sup>1</sup></div>
        </div>
        <div class="author-row">
            <div class="col-4 text-center"><a href="http://www.cim.mcgill.ca/~derek/">Derek Nowrouzezahrai</a><sup>3,5</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a><sup>2</sup></div>
            <div class="col-4 text-center"><a href="https://casual-effects.com/">Morgan McGuire</a><sup>1,3</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,4</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-5 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-5 text-center"><sup>2</sup>University of Toronto</div>
            <div class="col-5 text-center"><sup>3</sup>McGill University</div>
            <div class="col-5 text-center"><sup>4</sup>Vector Institute</div>
            <div class="col-5 text-center"><sup>5</sup>Mila</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2021 (Oral)</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="assets/nglod.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="https://www.youtube.com/watch?v=0cJZn_hV2Ms">
                <span class="material-icons"> videocam </span> 
                 Video
            </a>
            <a class="paper-btn" href="https://github.com/nv-tlabs/nglod">
                <span class="material-icons"> code </span> 
                 Code
            </a>
        </div></div>
    </div>

    <section id="teaser">
        <a href="assets/teaser.png">
            <img width="100%" src="assets/teaser_small.jpg">
        </a>
        <p class="caption">
            We are able to fit shapes of varying complexity, style, scale, with consistently 
            good quality, while being able to leverage the geometry for shading, ambient occlusion, and even 
            shadows with secondary rays.
        </p>
    </section>

    <section id="teaser-videos">
        <figure style="width: 33%; float: left">
            <video class="centered" width="100%" controls muted loop>
                <source src="assets/qualcomp.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
                Qualitative side-by-side comparisons of baselines (<span class="orange">orange</span>) against ours (<span class="blue">blue</span>).
            </p>
        </figure>
        
        <figure style="width: 33%; float: left">
            <video class="centered" width="100%" controls muted loop>
                <source src="assets/interp.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
                Comparison of no LOD blending (<span class="orange">orange</span>) vs LOD blending (<span class="blue">blue</span>).
            </p>
        </figure>
        
        <figure style="width: 33%; float: left">
            <video class="centered" width="100%" controls muted loop>
                <source src="assets/demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
                Real-time demo of our renderer.
            </p>
        </figure>
    </section>


    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> event </span> [May 2021] Our <a href="https://github.com/nv-tlabs/nglod">code</a> has been released.</div>
            <div><span class="material-icons"> event </span> [April 2021] Our work was presented at the <a href="https://toronto-geometry-colloquium.github.io/">Toronto Geometry Colloquium</a>. Check out the <a href="https://www.youtube.com/watch?v=Pi7W6XrFtMs">recording</a>!</div>
            <div><span class="material-icons"> event </span> [March 2021] Our paper was accepted as Oral Presentation to CVPR 2021!</div>
            <div><span class="material-icons"> event </span> [Jan 2021] <a href="https://twitter.com/yongyuanxi/status/1354478763065528320">Twitter thread</a> explaining our work in detail.</div>
            <div><span class="material-icons"> event </span> [Jan 2021] Project released on <a href="https://arxiv.org/abs/2101.10994">arXiv</a>.</div>
        </div>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <div style="width: 60%">
                <p>
                    Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. SDFs encode 3D surfaces with a function of position that returns the closest distance to a surface. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics applications.
                </p>
            </div>
            <div style="width: 40%">
                <figure style="padding-left: 24px; margin-bottom: 0">
                    <img width="100%" src="assets/nsdf.jpg">
                    <p class="caption">
                        Neural SDFs are typically encoded using large, fixed-size MLPs which are expensive to render.
                    </p>
                </figure>
            </div>
        </div>
        <p>
            We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.
            <figure style="margin-top: 20px; margin-bottom: 20px;">
                <img width="100%" src="assets/arch.jpg">
                <p class="caption">
                    We encode shapes using a sparse voxel octree (SVO) based feature volume, where the levels of the tree corresponds to LOD. This lets us use a very small MLP, which accelerates rendering by 2-3 orders of magnitudes.
                <p class="caption">
            </p>
        </figure>
    </section>

    <section id="results">
        <h2>Results</h2>
        <hr>
        <figure style="width: 600px;">
            <a href="assets/comparison.png">
                <img width="100%" src="assets/comparison.png">
            </a>
            <p class="caption" style="margin-bottom: 24px;">
                We qualitatively compare the mesh reconstructions. Only ours is able to recover fine details, with speeds 50&times; faster than FFN and comparable to NI. We render surface normals to highlight geometric details.
            </p>
        </figure>
        
        <figure style="width: 100%;">
            <a href="assets/sdf.png">
                <img width="100%" src="assets/sdf.png">
            </a>
            <p class="caption">
                We test against two difficult analytic SDF examples from Shadertoy; the Oldcar, which contains a highly non-metric signed distance field, as well as the Mandelbulb, which is a recursive fractal structure that can only be expressed using implicit surfaces. Only our architecture can reasonably reconstruct these hard cases. We render surface normals to highlight geometric details.
            </p>
        </figure>

        <figure>
            <video class="centered" width="100%" autoplay muted loop>
                <source src="assets/moreresult.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="caption">
                Additional reconstructions.
            </p>
        </figure>

    </section>
    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/nglod.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
            </div>
            <div style="width: 50%">
                <p><b>Neural Geometric Level of Detail: <br>Real-time Rendering with Implicit 3D Shapes</b></p>
                <p>Towaki Takikawa*, Joey Litalien*, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire & Sanja Fidler</p>
                <p><i>*Authors contributed equally</i></p>

                <div><span class="material-icons"> description </span><a href="assets/nglod.pdf"> Paper (PDF, 6.2 MB)</a></div>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2101.10994"> arXiv version</a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/nglod.bib"> BibTeX</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://github.com/nv-tlabs/nglod"> Code</a></div>
                <div><span class="material-icons"> videocam </span><a href="https://www.youtube.com/watch?v=0cJZn_hV2Ms"> Video (CVPR)</a></div>
                <div><span class="material-icons"> videocam </span><a href="https://www.youtube.com/watch?v=Pi7W6XrFtMs"> Talk (TGC)</a></div>

                <p>Please send feedback and questions to <a href="https://tovacinni.github.io">Towaki Takikawa</a></p>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{takikawa2021nglod,
    title = {Neural Geometric Level of Detail: Real-time Rendering with Implicit {3D} Shapes}, 
    author = {Towaki Takikawa and
              Joey Litalien and 
              Kangxue Yin and 
              Karsten Kreis and 
              Charles Loop and 
              Derek Nowrouzezahrai and 
              Alec Jacobson and 
              Morgan McGuire and 
              Sanja Fidler},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2021}
}
</code></pre>
    </section>

    <section id="acknowledgements">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="row">
            <p>We would like to thank 
            <a href="https://scholar.google.com/citations?user=PDvW5o4AAAAJ&hl=en">Jean-Francois Lafleche</a>, 
            <a href="https://www.petershirley.com/">Peter Shirley</a>, 
            <a href="https://kevincxie.github.io/">Kevin Xie</a>, 
            <a href="http://granskog.xyz/">Jonathan Granskog</a>, 
            <a href="https://research.nvidia.com/person/alex-evans">Alex Evans</a>, and 
            <a href="https://www.linkedin.com/in/alexbie98">Alex Bie</a> at NVIDIA for interesting discussions throughout the project. 
            We also thank 
            <a href="https://www.petershirley.com/">Peter Shirley</a>, 
            <a href="https://research.nvidia.com/person/zander-majercik">Alexander Majercik</a>, 
            <a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a>, 
            <a href="https://luebke.us/">David Luebke</a>,
            <a href="https://scholar.google.com/citations?user=VVIAoY0AAAAJ&hl=en">Jonah Philion</a> and 
            <a href="http://www.cs.toronto.edu/~jungao/">Jun Gao</a> for their help with paper editing.</p>
        </div>
    </section>
</div>
</body>
</html>

